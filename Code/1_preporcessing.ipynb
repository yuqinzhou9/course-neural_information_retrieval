{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading terrier-assemblies 5.x-SNAPSHOT jar-with-dependencies to /Users/zhouyuqin/.pyterrier...\n","Done\n"]},{"name":"stderr","output_type":"stream","text":["PyTerrier 0.8.1 has loaded Terrier 5.6 (built by jitpack on 2022-04-12 09:28)\n","\n","No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import json\n","import pyterrier as pt\n","from NIRfunction import *\n","if not pt.started():\n","    pt.init(version=\"snapshot\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Import data"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["train_query = get_query('../Project/NIR2022 dataset/train_query.csv')\n","train_qrel = get_qrel('../Project/NIR2022 dataset/train_qrel.csv')\n","corpus_pd = get_dataframe(\"../Project/NIR2022 dataset/corpus.jsonl\")\n","files = pt.io.find_files(\"../Project/Data\")"]},{"cell_type":"markdown","metadata":{},"source":["##### See basic infromation about the dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of documents: 528155\n","Number of terms: 840517\n","Number of postings: 125861126\n","Number of fields: 0\n","Number of tokens: 269905822\n","Field names: []\n","Positions:   false\n","\n"]}],"source":["# build the index\n","# indexer_TREC = pt.TRECCollectionIndexer(\"./indexes/stage1/index_TREC\",  verbose=True, blocks=False, overwrite=True) \n","# indexer_TREC.setProperties(**{\"termpipelines\":\"\"})\n","# index_ref_TREC = indexer_TREC.index(files)\n","\n","# Or load from files\n","index_ref_TREC = pt.IndexRef.of(\"./indexes/stage1/index_TREC\")\n","\n","# load the index, print the statistics\n","index_TREC = pt.IndexFactory.of(index_ref_TREC)\n","print(index_TREC.getCollectionStatistics().toString())"]},{"cell_type":"markdown","metadata":{},"source":["#### Test different tools of pre-processing"]},{"cell_type":"markdown","metadata":{},"source":["##### Pandas dataframe using EnglishSnowballStemmer, Stopwords"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of documents: 528155\n","Number of terms: 519118\n","Number of postings: 83894051\n","Number of fields: 0\n","Number of tokens: 143549248\n","Field names: []\n","Positions:   false\n","\n"]}],"source":["# build the index\n","# indexer_pd_s = pt.DFIndexer(\"./indexes/stage1/index_pd_s\",  verbose=True, blocks=False, overwrite= True) \n","# indexer_pd_s.setProperties(**{\"termpipelines\":\"EnglishSnowballStemmer, Stopwords\"})\n","# index_ref_pd_s = indexer_pd_s.index(corpus_pd[\"text\"], corpus_pd[\"docno\"])\n","\n","# Or load from files\n","index_ref_pd_s = pt.IndexRef.of(\"./indexes/stage1/index_pd_s/data.properties\")\n","\n","# load the index, print the statistics\n","index_pd_s = pt.IndexFactory.of(index_ref_pd_s)\n","print(index_pd_s.getCollectionStatistics().toString())"]},{"cell_type":"markdown","metadata":{},"source":["##### Pandas dataframe using EnglishSnowballStemmer"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of documents: 528155\n","Number of terms: 519484\n","Number of postings: 108169967\n","Number of fields: 0\n","Number of tokens: 250819280\n","Field names: []\n","Positions:   false\n","\n"]}],"source":["# build the index\n","# indexer_pd_snow = pt.DFIndexer(\"./indexes/stage1/index_pd_snow\",  verbose=True, blocks=False, overwrite= True) \n","# indexer_pd_snow.setProperties(**{\"termpipelines\":\"EnglishSnowballStemmer\"})\n","# index_ref_pd_snow = indexer_pd_snow.index(corpus_pd[\"text\"], corpus_pd[\"docno\"])\n","\n","# Or load from files\n","index_ref_pd_snow = pt.IndexRef.of(\"./indexes/stage1/index_pd_snow/data.properties\")\n","# load the index, print the statistics\n","index_pd_snow= pt.IndexFactory.of(index_ref_pd_snow)\n","print(index_pd_snow.getCollectionStatistics().toString())"]},{"cell_type":"markdown","metadata":{},"source":["##### Pandas dataframe using PorterStemmer"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of documents: 528155\n","Number of terms: 518432\n","Number of postings: 108055373\n","Number of fields: 0\n","Number of tokens: 250819280\n","Field names: []\n","Positions:   false\n","\n"]}],"source":["# build the index\n","# indexer_pd_porter = pt.DFIndexer(\"./indexes/stage1/index_pd_porter\",  verbose=True, blocks=False, overwrite= True) \n","# indexer_pd_porter.setProperties(**{\"termpipelines\":\"PorterStemmer\"})\n","# index_ref_pd_porter = indexer_pd_porter.index(corpus_pd[\"text\"], corpus_pd[\"docno\"])\n","\n","# Or load from files\n","index_ref_pd_porter = pt.IndexRef.of(\"./indexes/stage1/index_pd_porter/data.properties\")\n","\n","# load the index, print the statistics\n","index_pd_porter = pt.IndexFactory.of(index_ref_pd_porter)\n","print(index_pd_porter.getCollectionStatistics().toString())"]},{"cell_type":"markdown","metadata":{},"source":["##### Pandas dataframe using weakStemmer"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of documents: 528155\n","Number of terms: 557236\n","Number of postings: 110398660\n","Number of fields: 0\n","Number of tokens: 250819280\n","Field names: []\n","Positions:   false\n","\n"]}],"source":["#build the index\n","# indexer_pd_weak = pt.DFIndexer(\"./indexes/stage1/index_pd_weak\",  verbose=True, blocks=False, overwrite= True) \n","# indexer_pd_weak.setProperties(**{\"termpipelines\":\"WeakPorterStemmer\"})\n","# index_ref_pd_weak = indexer_pd_weak.index(corpus_pd[\"text\"], corpus_pd[\"docno\"])\n","\n","# Or load from files\n","index_ref_pd_weak = pt.IndexRef.of(\"./indexes/stage1/index_pd_weak/data.properties\")\n","\n","# load the index, print the statistics\n","index_pd_weak = pt.IndexFactory.of(index_ref_pd_weak)\n","print(index_pd_weak.getCollectionStatistics().toString())"]},{"cell_type":"markdown","metadata":{},"source":["##### Pandas dataframe without using Stopwords, Stemmer"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of documents: 528155\n","Number of terms: 619167\n","Number of postings: 115498835\n","Number of fields: 0\n","Number of tokens: 250819280\n","Field names: []\n","Positions:   false\n","\n"]}],"source":["# build the index\n","# indexer_pd_unstemmed = pt.DFIndexer(\"./indexes/stage1/index_pd_unstemmed\",  verbose=True, blocks=False, overwrite= True) \n","# indexer_pd_unstemmed .setProperties(**{\"termpipelines\":\"\"})\n","# index_ref_pd_unstemmed  = indexer_pd_unstemmed.index(corpus_pd[\"text\"], corpus_pd[\"docno\"])\n","\n","# Or load from files\n","index_ref_pd_unstemmed = pt.IndexRef.of(\"./indexes/stage1/index_pd_unstemmed/data.properties\")\n","\n","# load the index, print the statistics\n","index_pd_unstemmed = pt.IndexFactory.of(index_ref_pd_unstemmed)\n","print(index_pd_unstemmed.getCollectionStatistics().toString())"]},{"cell_type":"markdown","metadata":{},"source":["##### Pandas dataframe using lemmatization ?"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["# from nltk.stem import WordNetLemmatizer\n","# from nltk.tokenize import TreebankWordTokenizer\n","# import nltk\n","# lemmatizer = WordNetLemmatizer()\n","# tokenizer = TreebankWordTokenizer()\n","\n","# def lemmatize_text(text):\n","#     _a = \"\"\n","#     for w in tokenizer.tokenize(text):\n","#         _a += \" \" + lemmatizer.lemmatize(w)\n","#     return _a\n","\n","# corpus_pd_test = corpus_pd.copy()\n","\n","# corpus_pd_test[\"title\"] = corpus_pd_test[\"title\"].str.lower().replace('[^a-zA-Z]', ' ', regex=True).apply(lemmatize_text)\n","# corpus_pd_test[\"text\"] = corpus_pd_test[\"text\"].str.lower().replace('[^a-zA-Z]', ' ', regex=True).apply(lemmatize_text)\n","\n","# corpus_pd_csv = pd.read_csv(\"../Project/corpus.csv\")\n","# corpus_pd_csv = corpus_pd_csv.loc[:,\"docno\" : \"text\"]\n","\n","corpus_pd_csv = pd.read_csv(\"../Project/corpus.csv\")\n","corpus_pd_csv = corpus_pd_csv.loc[:,\"docno\" : \"text\"]"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>docno</th>\n","      <th>title</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FBIS3-1</td>\n","      <td>former yugoslav republic of macedonia opinion...</td>\n","      <td>politician party preference summary newspaper...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>FBIS3-2</td>\n","      <td>fbi medium guide the former yugoslavia</td>\n","      <td>introduction this guide is intended to help u...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>FBIS3-3</td>\n","      <td>pyongyang signal preference for talk with u s...</td>\n","      <td>summary pyongyang s response to the latest so...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>FBIS3-4</td>\n","      <td>algeria press abstract no</td>\n","      <td>the following summary highlight information f...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>FBIS3-5</td>\n","      <td>official medium play down ames case criticize...</td>\n","      <td>summary russian official have tried to minimi...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>528150</th>\n","      <td>LA123190-0130</td>\n","      <td>melvin durslag a crazy eight is too much for ...</td>\n","      <td>december monday p m final inspecting the bowl...</td>\n","    </tr>\n","    <tr>\n","      <th>528151</th>\n","      <td>LA123190-0131</td>\n","      <td>short take mcferrin in hour songfest</td>\n","      <td>december monday p m final vocalist bobby mcfe...</td>\n","    </tr>\n","    <tr>\n","      <th>528152</th>\n","      <td>LA123190-0132</td>\n","      <td>short take martin sheen spawn news</td>\n","      <td>december monday p m final actor martin sheen ...</td>\n","    </tr>\n","    <tr>\n","      <th>528153</th>\n","      <td>LA123190-0133</td>\n","      <td>liz smith ruling in buchwald suit shake studi...</td>\n","      <td>december monday p m final in the summer of th...</td>\n","    </tr>\n","    <tr>\n","      <th>528154</th>\n","      <td>LA123190-0134</td>\n","      <td>short take tammy see country s rebirth</td>\n","      <td>december monday p m final tammy wynette say a...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>528155 rows Ã— 3 columns</p>\n","</div>"],"text/plain":["                docno                                              title  \\\n","0             FBIS3-1   former yugoslav republic of macedonia opinion...   \n","1             FBIS3-2             fbi medium guide the former yugoslavia   \n","2             FBIS3-3   pyongyang signal preference for talk with u s...   \n","3             FBIS3-4                          algeria press abstract no   \n","4             FBIS3-5   official medium play down ames case criticize...   \n","...               ...                                                ...   \n","528150  LA123190-0130   melvin durslag a crazy eight is too much for ...   \n","528151  LA123190-0131               short take mcferrin in hour songfest   \n","528152  LA123190-0132                 short take martin sheen spawn news   \n","528153  LA123190-0133   liz smith ruling in buchwald suit shake studi...   \n","528154  LA123190-0134             short take tammy see country s rebirth   \n","\n","                                                     text  \n","0        politician party preference summary newspaper...  \n","1        introduction this guide is intended to help u...  \n","2        summary pyongyang s response to the latest so...  \n","3        the following summary highlight information f...  \n","4        summary russian official have tried to minimi...  \n","...                                                   ...  \n","528150   december monday p m final inspecting the bowl...  \n","528151   december monday p m final vocalist bobby mcfe...  \n","528152   december monday p m final actor martin sheen ...  \n","528153   december monday p m final in the summer of th...  \n","528154   december monday p m final tammy wynette say a...  \n","\n","[528155 rows x 3 columns]"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["corpus_pd_csv"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of documents: 528029\n","Number of terms: 513770\n","Number of postings: 105833984\n","Number of fields: 0\n","Number of tokens: 241926225\n","Field names: []\n","Positions:   false\n","\n"]}],"source":["# build the index\n","# indexer_pd_lem = pt.DFIndexer(\"./indexes/stage1/index_pd_lem\",  verbose=True, blocks=False, overwrite= True) \n","# indexer_pd_lem.setProperties(**{\"termpipelines\":\"\"})\n","# index_ref_pd_lem  = indexer_pd_lem.index(corpus_pd_csv[\"text\"], corpus_pd_csv[\"docno\"])\n","\n","# Or load from files\n","index_ref_pd_lem = pt.IndexRef.of(\"./indexes/stage1/index_pd_lem/data.properties\")\n","\n","# load the index, print the statistics\n","index_pd_lem = pt.IndexFactory.of(index_ref_pd_lem)\n","print(index_pd_lem.getCollectionStatistics().toString())"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>P_10</th>\n","      <th>map</th>\n","      <th>ndcg_cut_5</th>\n","      <th>ndcg_cut_10</th>\n","      <th>ndcg_cut_20</th>\n","      <th>mrt</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>BM25_snowball + stop words</td>\n","      <td>0.415075</td>\n","      <td>0.236816</td>\n","      <td>0.447339</td>\n","      <td>0.427750</td>\n","      <td>0.403174</td>\n","      <td>19.370199</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>BM25_snowball</td>\n","      <td>0.402010</td>\n","      <td>0.230514</td>\n","      <td>0.436028</td>\n","      <td>0.415865</td>\n","      <td>0.392624</td>\n","      <td>19.545578</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>BM25_porter</td>\n","      <td>0.398492</td>\n","      <td>0.229417</td>\n","      <td>0.431294</td>\n","      <td>0.411652</td>\n","      <td>0.390447</td>\n","      <td>19.426615</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>BM25_weakporter</td>\n","      <td>0.395477</td>\n","      <td>0.222997</td>\n","      <td>0.428788</td>\n","      <td>0.408978</td>\n","      <td>0.389292</td>\n","      <td>18.687542</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>BM25_unstemmed</td>\n","      <td>0.375377</td>\n","      <td>0.204104</td>\n","      <td>0.411441</td>\n","      <td>0.388466</td>\n","      <td>0.369087</td>\n","      <td>17.873132</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>BM25_lem</td>\n","      <td>0.338693</td>\n","      <td>0.193131</td>\n","      <td>0.363024</td>\n","      <td>0.346245</td>\n","      <td>0.331853</td>\n","      <td>17.124453</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         name      P_10       map  ndcg_cut_5  ndcg_cut_10  \\\n","0  BM25_snowball + stop words  0.415075  0.236816    0.447339     0.427750   \n","1               BM25_snowball  0.402010  0.230514    0.436028     0.415865   \n","2                 BM25_porter  0.398492  0.229417    0.431294     0.411652   \n","3             BM25_weakporter  0.395477  0.222997    0.428788     0.408978   \n","4              BM25_unstemmed  0.375377  0.204104    0.411441     0.388466   \n","5                    BM25_lem  0.338693  0.193131    0.363024     0.346245   \n","\n","   ndcg_cut_20        mrt  \n","0     0.403174  19.370199  \n","1     0.392624  19.545578  \n","2     0.390447  19.426615  \n","3     0.389292  18.687542  \n","4     0.369087  17.873132  \n","5     0.331853  17.124453  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["BM25_pd_snowball_s = pt.BatchRetrieve(index_pd_s, wmodel=\"BM25\")\n","BM25_pd_snowball = pt.BatchRetrieve(index_pd_snow, wmodel=\"BM25\")\n","BM25_pd_porter = pt.BatchRetrieve(index_pd_porter, wmodel=\"BM25\")\n","BM25_pd_weakporter = pt.BatchRetrieve(index_pd_weak, wmodel=\"BM25\")\n","BM25_pd_unstemmed = pt.BatchRetrieve(index_pd_unstemmed, wmodel=\"BM25\")\n","BM25_pd_lem = pt.BatchRetrieve(index_ref_pd_lem, wmodel=\"BM25\")\n","\n","pt.Experiment([BM25_pd_snowball_s, BM25_pd_snowball, BM25_pd_porter, BM25_pd_weakporter, BM25_pd_unstemmed, BM25_pd_lem], \n","                train_query, \n","                train_qrel, \n","                eval_metrics=[\"P_10\", \"map\", \"ndcg_cut_5\", \"ndcg_cut_10\", \"ndcg_cut_20\", \"mrt\"],\n","                names=[\"BM25_snowball + stop words\", \"BM25_snowball\", \"BM25_porter\", \"BM25_weakporter\", \"BM25_unstemmed\", \"BM25_lem\"])"]},{"cell_type":"markdown","metadata":{},"source":["##### Further information about the dataset"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["5 2\n"]}],"source":["# how many u s\n","u_s = 0\n","for i in train_query[\"query\"].values:\n","    if \"u s\" in i:\n","        u_s += 1\n","\n","# how many 's\n","_s_ = 0\n","for i in train_query[\"query\"].values:\n","    if \" s \" in i and 'u s' not in i:\n","        _s_ += 1\n","\n","print(u_s, _s_)"]},{"cell_type":"markdown","metadata":{},"source":["##### Topic length"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["u s invasion of panama\n","non u s media bias\n","2.71 5 1\n"]}],"source":["max_topic_len = 0\n","min_topic = 10\n","average_topic = 0\n","\n","# find longest topic\n","for i in train_query[\"query\"].values:\n","    if len(i.split()) > max_topic_len:\n","        max_topic = i.split()\n","        max_topic_len = len(i.split())\n","\n","# find shorters topic\n","for i in train_query[\"query\"].values:\n","    if len(i.split()) < min_topic:\n","        min_topic = i.split()\n","        min_topic = len(i.split())\n","\n","\n","# find shorters topic\n","for i in train_query[\"query\"].values:\n","    if len(i.split()) == 5:\n","        print(i)\n","\n","\n","\n","# find average topic length\n","_a= 0\n","for i in train_query[\"query\"].values:\n","    _a += len(i.split())\n","# minus -1 for mistoknenized words \"U.S\" and \"'s\"\n","average_topic = (_a - u_s - _s_ )/ len(train_query[\"query\"])\n","\n","print(average_topic, max_topic_len, min_topic)"]},{"cell_type":"markdown","metadata":{},"source":["##### Document length"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of documents: 528155\n","Number of terms: 739349\n","Number of postings: 93025387\n","Number of fields: 0\n","Number of tokens: 159579026\n","Field names: []\n","Positions:   false\n","\n"]}],"source":["# build the index\n","# indexer_TREC_stemmed = pt.TRECCollectionIndexer(\"./indexes/stage1/index_TREC_stemmed\",  verbose=True, blocks=False, overwrite=True) \n","# indexer_TREC_stemmed.setProperties(**{\"termpipelines\":\"EnglishSnowballStemmer, Stopwords\"})\n","# index_ref_TREC_stemmed = indexer_TREC_stemmed.index(files)\n","\n","# Or load from files\n","index_ref_TREC_stemmed = pt.IndexRef.of(\"./indexes/stage1/index_TREC_stemmed\")\n","\n","# load the index, print the statistics\n","index_TREC_stemmed = pt.IndexFactory.of(index_ref_TREC_stemmed)\n","print(index_TREC_stemmed.getCollectionStatistics().toString())"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["max_doc_len = 0\n","min_doc_len = 1000000000000000\n","_a = 0\n","\n","for i in range(528155):\n","    _j = index_TREC_stemmed.getDocumentIndex().getDocumentLength(i)\n","    _a += _j\n","    if _j > max_doc_len:\n","        max_doc_len = _j\n","    if _j < min_doc_len:\n","        min_doc_len = _j\n","average_doc_per_topic = _a / 528155"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["184423 1 302.1443061222558\n"]}],"source":["print(max_doc_len, min_doc_len, average_doc_per_topic)"]},{"cell_type":"markdown","metadata":{},"source":["##### Assessed documents per topic"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["max_doc_per_topic = 0\n","min_doc_per_topic = 1000000\n","_b = 0\n","\n","for i in set(train_qrel[\"qid\"].values):\n","    _j = len(train_qrel[train_qrel[\"qid\"] == i])\n","    _b += _j\n","    if _j > max_doc_per_topic:\n","        max_doc_per_topic = _j\n","    if _j < min_doc_per_topic:\n","        min_doc_per_topic = _j"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["# Number of valid queries\n","len(set(train_qrel[\"qid\"].values))\n","\n","# average assessed documents per topic\n","average_doc_per_topic = _b / len(set(train_qrel[\"qid\"].values))"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2992 207 1244.0653266331658\n"]}],"source":["print(max_doc_per_topic, min_doc_per_topic, average_doc_per_topic)"]},{"cell_type":"markdown","metadata":{},"source":["##### Term frequency length"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["max_word = 0\n","min_word = 10000000000000\n","_c = 0\n","\n","for i in index_TREC_stemmed.getLexicon():\n","    j = index_TREC_stemmed.getLexicon()[i.getKey()]\n","    _j = j.getFrequency()\n","    _c += _j\n","    if _j > max_word:\n","        max_word = _j\n","    if _j < min_word:\n","        min_word = _j\n"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[{"data":{"text/plain":["215.83721084359348"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["_c / 739349"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1041512 1\n"]}],"source":["print(max_word, min_word)"]}],"metadata":{"interpreter":{"hash":"81c0a520ab9d5f38718f11fc8e39c528dfdc4be4c6d24f2eb9940d412ba4096a"},"kernelspec":{"display_name":"Python 3.8.13 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
